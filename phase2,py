import os
import re
from collections import Counter
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from fuzzywuzzy import process


def load_documents(directory):
    """
    Load all .txt documents from the specified directory.
    """
    documents = {}
    for filename in os.listdir(directory):
        if filename.endswith(".txt"):
            file_path = os.path.join(directory, filename)
            with open(file_path, 'r', encoding='utf-8') as file:
                documents[filename] = file.read()
    return documents


def preprocess_text(text):
    """
    Preprocess the text by converting to lowercase and removing special characters.
    """
    text = text.lower()
    text = re.sub(r'[^a-z0-9\s]', '', text)
    return text


def extract_common_phrases(doc1, doc2, top_n=10):
    """
    Find common words or phrases between two documents using cosine similarity.
    """
    vectorizer = CountVectorizer(ngram_range=(1, 3)).fit([doc1, doc2])
    vectors = vectorizer.transform([doc1, doc2]).toarray()
    similarity = cosine_similarity(vectors)
    
    # Extract terms from both documents
    terms = vectorizer.get_feature_names_out()
    term_frequencies = Counter(doc1.split()) + Counter(doc2.split())
    
    # Fuzzy match the most frequent terms
    matched_phrases = []
    for term in terms:
        matches = process.extract(term, terms, limit=top_n)
        matched_phrases.extend([match[0] for match in matches if match[1] > 80])  # Threshold for similarity
    
    # Remove duplicates
    matched_phrases = list(set(matched_phrases))
    
    return matched_phrases, similarity[0][1]


def find_common_phrases_across_documents(documents):
    """
    Compare all pairs of documents and find common phrases.
    """
    results = []
    filenames = list(documents.keys())
    for i in range(len(filenames)):
        for j in range(i + 1, len(filenames)):
            doc1 = preprocess_text(documents[filenames[i]])
            doc2 = preprocess_text(documents[filenames[j]])
            common_phrases, similarity = extract_common_phrases(doc1, doc2)
            results.append({
                'doc1': filenames[i],
                'doc2': filenames[j],
                'common_phrases': common_phrases,
                'cosine_similarity': similarity
            })
    return results


if __name__ == "__main__":
    # Directory containing filtered documents
    directory = "./documents_filtered"

    # Load and preprocess documents
    documents = load_documents(directory)

    # Find common phrases across all documents
    results = find_common_phrases_across_documents(documents)

    # Output results
    for result in results:
        print(f"Documents: {result['doc1']} & {result['doc2']}")
        print(f"Cosine Similarity: {result['cosine_similarity']:.4f}")
        print("Common Phrases:")
        for phrase in result['common_phrases']:
            print(f"  - {phrase}")
        print("-" * 50)
